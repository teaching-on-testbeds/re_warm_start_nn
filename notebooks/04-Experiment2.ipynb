{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2:\n",
    "\n",
    "In this experiment, we evaluate the claim that *â€œThe test accuracy of the warm-started model is lower than that of the randomly initialized model across various datasets, optimizers and model architecturesâ€*. We experiment with different combinations of model architectures, datasets and optimizers and expect to see a gap in test accuracy between the warm-started and random initialized models in each case.\n",
    "\n",
    "We compare two methods of weight initialization: warm-starting and random initialization, for two models: **ResNet18** and **3-layer MLP** with tanh activation. We also compare two optimizers: **SGD** and **Adam**, for updating the weights based on the gradients. We use three image classification datasets: **CIFAR-10**, **CIFAR-100** and **SVHN**, and report the test accuracy of each model on each dataset. All models are trained using a mini-batch size of 128 and a learning rate of 0.001.\n",
    "\n",
    "We use the same components as in experiment one: the `get_loaders` function to get the required datasetâ€™s train and test loaders, the [**ResNet18**](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) model from `torchvision.models`, and the [**SGD**](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer from `torch.optim`."
   ],
   "id": "c9ca2950-94a8-4ce5-92ab-c050859848ba"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We import the required packages as before."
   ],
   "id": "82cc6d8d-8b58-4780-95b3-c0be5336cb8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms, datasets, models"
   ],
   "id": "489acfeb-f0ba-46c7-ab5d-23db18b98143"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "This is the same `get_loaders` function from Experiment 1"
   ],
   "id": "785d4645-65ef-4e76-9a0e-afb64abe5085"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(use_half_train=False, batch_size=128, dataset_portion=None):\n",
    "    \"\"\"\n",
    "    This loads the whole CIFAR-10 into memory and returns train and test data according to params\n",
    "    @param use_half_train (bool): return half the data or the whole train data\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "    @param dataset_portion (double): portion of train data\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "        \n",
    "    # Test transformation function    \n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "    \n",
    "    # Load data from torchvision datasets\n",
    "    original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                         train=True, transform=train_transform, download=True)\n",
    "    original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                         train=False, transform=test_transform, download=True)\n",
    "    \n",
    "    # Check half data flag\n",
    "    if use_half_train:\n",
    "        print('Using Half Data')\n",
    "        dataset_portion = 0.5\n",
    "        \n",
    "    # Check if only a portion is required\n",
    "    if dataset_portion:\n",
    "        dataset_size = len(original_train_dataset)\n",
    "        split = int(np.floor((1 - dataset_portion) * dataset_size))\n",
    "        original_train_dataset, _ = random_split(original_train_dataset, [dataset_size - split, split])\n",
    "    \n",
    "    # Creating data loaders\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=False,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "e5f206bb-5a14-4c31-a777-c00e1fa52380"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We will train the model until it reaches the threshold, not a fixed number of epochs. Therefore, we define two functions in the following cell that perform one epoch of training or evaluation on a data loader and keep calling them in the training function until we reach the threshold. They return the average loss and accuracy of the model."
   ],
   "id": "9bd08247-029b-4cc5-bb40-50f786b90c35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes predictions and true values to return accuracies\n",
    "def get_accuracy(logit, true_y):\n",
    "    pred_y = torch.argmax(logit, dim=1)\n",
    "    return (pred_y == true_y).float().mean()\n",
    "\n",
    "def eval_on_dataloader(device, criterion, model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given data loader and return the average loss and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    device: the device (cpu or gpu) to use for computation\n",
    "    criterion: the loss function to use\n",
    "    model: the model to evaluate\n",
    "    dataloader: the data loader to iterate over the data\n",
    "\n",
    "    Returns:\n",
    "    loss: the average loss over the data loader\n",
    "    accuracy: the average accuracy over the data loader\n",
    "    \"\"\"\n",
    "    # Lists to store accuracy and loss\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx, (data_x, data_y) in enumerate(dataloader): \n",
    "        data_x = data_x.to(device) \n",
    "        data_y = data_y.to(device)\n",
    "        \n",
    "        # get the model output for the input data\n",
    "        model_y = model(data_x) \n",
    "        \n",
    "        # compute the loss and accuracy\n",
    "        loss = criterion(model_y, data_y)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "        \n",
    "        # append accuracy and loss to lists\n",
    "        accuracies.append(batch_accuracy.item()) \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # compute average loss and accuracy\n",
    "    loss = np.mean(losses) \n",
    "    accuracy = np.mean(accuracies) \n",
    "    return loss, accuracy \n",
    "\n",
    "\n",
    "def train_one_epoch(device, model, optimizer, criterion, dataloader):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch on a given training data loader and return the average loss and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    device: the device (cpu or gpu) to use for computation\n",
    "    model: the model to train\n",
    "    optimizer: the optimizer to use for updating the weights\n",
    "    criterion: the loss function to use\n",
    "    train_dataloader: the training data loader to iterate over the training data\n",
    "\n",
    "    Returns:\n",
    "    train_loss: the average loss over the training data loader\n",
    "    train_accuracy: the average accuracy over the training data loader\n",
    "    \"\"\"\n",
    "    # Lists to store accuracy and loss\n",
    "    accuracies = []\n",
    "    losses = [] \n",
    "    \n",
    "    for batch_idx, (data_x, data_y) in enumerate(dataloader):\n",
    "        data_x = data_x.to(device) \n",
    "        data_y = data_y.to(device) \n",
    "        \n",
    "         # reset the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the model output for the input data\n",
    "        model_y = model(data_x)\n",
    "        \n",
    "        # compute the loss and accuracy\n",
    "        loss = criterion(model_y, data_y)\n",
    "        batch_accuracy = get_accuracy(model_y, data_y)\n",
    "        \n",
    "        # compute the gradients and update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # append accuracy and loss to lists\n",
    "        accuracies.append(batch_accuracy.item()) \n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # compute average loss and accuracy\n",
    "    loss = np.mean(losses) \n",
    "    accuracy = np.mean(accuracies) \n",
    "    return loss, accuracy \n"
   ],
   "id": "64e2d2d2-0724-450a-9583-1ae4f33bc0b0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We train our models to convergence rather than a fixed number of epochs. This allows models with different capacities to learn at their own pace. For instance, a model might reach its optimal performance at 100 epochs, while another one might still improve after 200 epochs. By training to convergence, we avoid underfitting or overfitting our models.\n",
    "\n",
    "We introduce the `train_model_threshold` function which is the same as `train_model_epochs` functions except that instead of training for a certain number of epochs we train until:\n",
    "\n",
    "-   The training accuracy of the model is equal to the `training_threshold`\n",
    "-   The change in the training accuracy doesnâ€™t exceed `convergence_change_threshold` for certain number of epochs specified by `convergence_epochs` parameter"
   ],
   "id": "3610eac3-e133-4e73-acb0-e6de8684b23f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_threshold(title='warm', lr=0.001, checkpoint=None, use_half_data=False, convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    model = models.resnet18(num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} \" \\\n",
    "        f\"ResNet-18 model with SGD optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    model_name = 'resnet18'\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-sgd'\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100 "
   ],
   "id": "758aa140-0298-4045-978b-001d30a8f88e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Before running the experiment we create a parameter table for to store the parameter values from the paper that we will use in the upcoming cells.\n",
    "\n",
    "| Dataset ðŸ†• |    Model ðŸ†•     | Optimizer ðŸ†• | Learning rate | Train threshold ðŸ†• |\n",
    "|:---------:|:--------------:|:-----------:|:------------:|:-----------------:|\n",
    "|  CIFAR-10  | ResNet-18 / MLP |  SGD / Adam  |    0.0001     |        99%         |\n",
    "| CIFAR-100  | ResNet-18 / MLP |  SGD / Adam  |    0.0001     |        99%         |\n",
    "|    SVHN    | ResNet-18 / MLP |  SGD / Adam  |    0.0001     |        99%         |\n",
    "\n",
    "We will extend our functions as we go to run the whole experiment. For now, they support **CIFAR-10**, **SGD** and **ResNet-18**."
   ],
   "id": "d491fc0e-31cc-4306-93fc-04da78bc2b4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to save all results\n",
    "overall_results = {}\n",
    "\n",
    "# train on full data with random initialization\n",
    "random_init = train_model_threshold(title='resnet-sgd-cifar10', train_threshold=0.99)"
   ],
   "id": "743300ca-0ad0-4d05-81a6-20236da78835"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on half data\n",
    "_ = train_model_threshold(title='resnet-sgd-cifar10', train_threshold=0.99, use_half_data=True)\n",
    "\n",
    "# train on full data with warm-starting\n",
    "warm_start = train_model_threshold(title='resnet-sgd-cifar10', train_threshold=0.99,\n",
    "                                     checkpoint='experiments/exp2/resnet-sgd-cifar10/resnet18-sgd.pt')"
   ],
   "id": "39f9aa0b-b5d6-4596-bfb3-217eec39fd68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the difference between random and warm-start models using sgd optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['resnet-sgd-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "ba64cfd2-807a-42d9-bb1d-fd1346c1acbe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We extend this experiment by training the same model with the Adam optimizer instead of SGD. We add a new parameter `optimizer_name` to select the optimizer for the model. The `torch.optim.Adam` class that implements the [**Adam**](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer, which is an adaptive learning rate method."
   ],
   "id": "f6b215c7-cd72-4c20-86e5-6e5bbec235ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_threshold(title='warm', lr=0.001, checkpoint=None, \n",
    "                       use_half_data=False, optimizer_name='adam', convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    model = models.resnet18(num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} ResNet-18 model \" \\\n",
    "            f\"with {optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    model_name = 'resnet18'\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "ae034733-36cc-471b-94df-dcbc7039e2cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat the training of the same models with the **Adam** optimizer instead of **SGD**."
   ],
   "id": "e2551142-dc8f-46df-95ba-7f2c17409788"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on full data with random initialization but with Adam\n",
    "random_init = train_model_threshold(title='resnet-adam-cifar10', train_threshold=0.99, optimizer_name='adam')\n",
    "\n",
    "# train on half data\n",
    "_ = train_model_threshold(title='resnet-adam-cifar10', train_threshold=0.99,\n",
    "                                             optimizer_name='adam', use_half_data=True)\n",
    "\n",
    "# train on full data with warm-starting but with Adam\n",
    "warm_start = train_model_threshold(title='resnet-adam-cifar10', train_threshold=0.99, optimizer_name='adam',\n",
    "                                        checkpoint='experiments/exp2/resnet-adam-cifar10/resnet18-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start models using Adam optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['resnet-adam-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "74be155a-ead1-4646-a485-2a513cfea463"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We introduce a new class for a **multilayer perceptron** (MLP) model with several fully connected (fc) layers and a final fully connected layer for the logits output. The arguments are:\n",
    "\n",
    "-   `input_dim`: the input feature dimension.\n",
    "-   `num_classes`: the output class number.\n",
    "-   `hidden_units`: the hidden unit number for each fc layer we set the default as 100 dimension as mentioned in the appendix.\n",
    "-   `activation`: the activation function, either `tanh` or `relu`.\n",
    "-   `bias`: whether to use bias terms in the fc layers.\n",
    "\n",
    "The function returns an MLP model object that can be trained or tested. The forward method takes an input tensor x and returns an output tensor x with the logits values. The output tensor does not have a final activation function. This will be used to create the **3-layer MLP** model."
   ],
   "id": "9ac0113f-e136-4749-b916-712c1bb6005c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for a multilayer perceptron (MLP) model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=10, hidden_units=[100, 100, 100], activation='tanh', bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Check that the activation argument is valid\n",
    "        assert activation in ['tanh', 'relu'], \"Activation must be tanh or relu\"\n",
    "\n",
    "        # Assign the activation function based on the argument\n",
    "        if activation == 'tanh':\n",
    "            self.activation_function = torch.tanh\n",
    "        if activation == 'relu':\n",
    "            self.activation_function = torch.relu\n",
    "        \n",
    "        # Store num_classes and input_dim to be used in forward function\n",
    "        self.num_classes = num_classes\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Initialize a variable to keep track of the last dimension of the layers\n",
    "        last_dim = input_dim\n",
    "        \n",
    "        # Initialize an empty list to store the fully connected (fc) layers\n",
    "        self.fcs = []\n",
    "        \n",
    "        # Loop through the hidden units argument and create fc layers with the given dimensions and bias\n",
    "        for i, n_h in enumerate(hidden_units):\n",
    "            self.fcs.append(nn.Linear(last_dim, n_h, bias=bias))\n",
    "            # Register the fc layer as a submodule with a name\n",
    "            self.add_module(f\"hidden_layer_{i}\", self.fcs[-1])\n",
    "            # Update the last dimension to match the output dimension of the fc layer\n",
    "            last_dim = n_h\n",
    "            \n",
    "        # Create a final fc layer for the logits output with the number of classes and bias\n",
    "        self.logit_fc = nn.Linear(last_dim, self.num_classes, bias=bias)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape the input x to have a batch size and an input dimension\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        \n",
    "        # Loop through the fc layers and apply them to x with the activation function\n",
    "        for fc in self.fcs:\n",
    "            x = fc(x)\n",
    "            x = self.activation_function(x)\n",
    "            \n",
    "        # Apply the final fc layer to x and return it as the output\n",
    "        x = self.logit_fc(x)\n",
    "        \n",
    "        # x is returned without adding the final activation\n",
    "        return x"
   ],
   "id": "095668a7-e3a6-48ab-a182-1ea5ab1a345b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We also experiment with the **MLP** model instead of the **ResNet**. We introduce a parameter `model_name` to select the model for the training."
   ],
   "id": "848d5424-b928-44cf-a751-706436fe8af7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_threshold(title='warm', lr=0.001, checkpoint=None, use_half_data=False,\n",
    "                        optimizer_name='adam', model_name='resnet18', convergence_epochs=4,\n",
    "                        train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(num_classes=10).to(device)\n",
    "    else:\n",
    "        model = MLP( input_dim = 32 * 32 * 3, num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'warm-starting' if checkpoint is not None else 'random initialized'} {model_name} model \" \\\n",
    "            f\"with {optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of cifar10 dataset\")\n",
    "\n",
    "    # initialize training varaibles\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save model if it is needed for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        \n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "67bb91f3-5a81-4a38-81d8-cf89f7aa7270"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We use warm-starting and the **SGD** optimizer to train the **MLP** model in the next cell."
   ],
   "id": "103a3380-a778-4466-9ca1-79abe177acb1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full data with random initialization and SGD optimizer\n",
    "random_init = train_model_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, \n",
    "                                 optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train MLP model on half data\n",
    "_ = train_model_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP on full data with warm-starting and SGD optimizer\n",
    "warm_start = train_model_threshold(title='mlp-sgd-cifar10', train_threshold=0.99, optimizer_name='sgd', \n",
    "                                model_name='mlp', checkpoint='experiments/exp2/mlp-sgd-cifar10/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['mlp-sgd-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "be053cca-2989-453b-a81a-538e478b093e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "In the next cell, we train the **MLP** model with warm-starting and the **Adam** optimizer."
   ],
   "id": "d8d83c42-1036-470a-83dc-f390aed27090"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full data with random initialization\n",
    "random_init= train_model_threshold(title='mlp-adam-cifar10', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train MLP mode on half data\n",
    "_ = train_model_threshold(title='mlp-adam-cifar10', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP on full data with warm-starting\n",
    "warm_start = train_model_threshold(title='mlp-adam-cifar10', train_threshold=0.99, optimizer_name='adam',\n",
    "                                model_name='mlp', checkpoint='experiments/exp2/mlp-adam-cifar10/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on CIFAR-10\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['mlp-adam-cifar10'] = [random_init, warm_start, diff]"
   ],
   "id": "6af5642d-7acd-46a6-91ec-3f975f1cd1ea"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We extend the `get_loaders` function to include two more datasets:\n",
    "\n",
    "1.  The **CIFAR-100** dataset, which has 60,000 color images of 100 classes. To get the CIFAR-100 dataset, we pass the string `cifar100` as the dataset name argument to the `get_loaders` function.\n",
    "2.  The **SVHN** dataset, which has 73,257 color images of 10 classes of street view house numbers. To get the SVHN dataset, we pass the string `svhn` as the dataset name argument to the `get_loaders` function."
   ],
   "id": "48101fa0-35c4-43d1-a993-4acd829279fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(dataset=\"cifar10\", use_half_train=False, batch_size=128, dataset_portion=None):\n",
    "    \"\"\"\n",
    "    This loads the whole CIFAR-10 into memory and returns train and test data according to params\n",
    "    @param use_half_train (bool): return half the data or the whole train data\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "    @param dataset_portion (double): portion of train data\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "        \n",
    "    # Test transformation function    \n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "    \n",
    "    # Check which dataset is required and load data from torchvision datasets\n",
    "    if dataset == 'cifar10':\n",
    "        original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == 'cifar100':\n",
    "        original_train_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=True, transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.CIFAR100(root=os.path.join('data', 'cifar100_data'),\n",
    "                                             train=False, transform=test_transform, download=True)\n",
    "    elif dataset == 'svhn':\n",
    "        original_train_dataset = datasets.SVHN(root=os.path.join('data', 'svhn_data'),\n",
    "                                             split='train', transform=train_transform, download=True)\n",
    "        original_test_dataset = datasets.SVHN(root=os.path.join('data', 'svhn_data'),\n",
    "                                             split='test', transform=test_transform, download=True)\n",
    "    \n",
    "    # Check half data flag\n",
    "    if use_half_train:\n",
    "        print('Using Half Data')\n",
    "        dataset_portion = 0.5\n",
    "        \n",
    "    # Check if only a portion is required\n",
    "    if dataset_portion:\n",
    "        dataset_size = len(original_train_dataset)\n",
    "        split = int(np.floor((1 - dataset_portion) * dataset_size))\n",
    "        original_train_dataset, _ = random_split(original_train_dataset, [dataset_size - split, split])\n",
    "    \n",
    "    # Creating data loaders\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=False,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "94fcb16b-9ea2-4378-b17f-3eb90f29acaf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Finaly we extend this to allow using the **SVHN** and **CIFAR-100** datasets by adding a parameter `dataset` to specify the dataset we would like to use."
   ],
   "id": "37f86951-edc9-41d4-9ff7-d8f52cbf2418"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_threshold(title='warm', dataset='cifar10', lr=0.001, checkpoint=None, use_half_data=False,\n",
    "                       optimizer_name='adam', model_name='resnet18', convergence_epochs=4,\n",
    "                       train_threshold=0.5, convergence_change_threshold=0.002, random_seed=42):\n",
    "    # use gpu if available ( change device id if needed )\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(dataset=dataset, use_half_train=use_half_data)\n",
    "\n",
    "    # Define the number of classes\n",
    "    num_classes = 10\n",
    "    if dataset == 'cifar100':\n",
    "        num_classes=100\n",
    "\n",
    "    # Get the model\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(num_classes=num_classes).to(device)\n",
    "    else:\n",
    "        model = MLP( input_dim = 32 * 32 * 3, num_classes=num_classes).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    print(f\"Training {'random initialized' if checkpoint is None else 'warm-starting'} {model_name} model wtih \" \\\n",
    "            f\"{optimizer_name.upper()} optimizer on {'50%' if use_half_data else '100%'} of {dataset} dataset\")\n",
    "\n",
    "    # initialize training variables\n",
    "    train_accuracies = []\n",
    "    stop_indicator = False\n",
    "    epoch = 0\n",
    "    # Train until convergence or stop condition is met\n",
    "    while(not stop_indicator):\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"\\t Training in epoch {epoch + 1} \\t\")\n",
    "        # Train for one epoch and get loss and accuracy\n",
    "        train_loss, train_accuracy = train_one_epoch(device, model, optimizer, criterion, loaders['train_loader'])\n",
    "\n",
    "        # Append training accuracy to list\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        epoch += 1\n",
    "        # Check if training accuracy is above a threshold\n",
    "        if train_accuracy >= train_threshold:\n",
    "            print(f\"Convergence codition met. Training accuracy > {train_threshold}\")\n",
    "            stop_indicator = True\n",
    "\n",
    "        # Check if training accuracy has stopped improving for a number of epochs\n",
    "        if len(train_accuracies) >= convergence_epochs:\n",
    "            if np.std(train_accuracies[-convergence_epochs:]) < convergence_change_threshold:\n",
    "                print(f\"\\tConvergence codition met. Training accuracy = {train_accuracy} stopped improving\")\n",
    "                stop_indicator = True\n",
    "\n",
    "    # Evaluate on test set and get loss and accuracy\n",
    "    test_loss, test_accuracy =  eval_on_dataloader(device, criterion, model, loaders['test_loader'])\n",
    "    print(f\"\\tTest accuracy = {test_accuracy}\")\n",
    "\n",
    "    # Save the model if it will be used for warm-starting\n",
    "    if use_half_data:\n",
    "        # Create directory in exp with experiment title\n",
    "        experiment_dir = os.path.join('experiments/exp2', title)\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "        # save the model\n",
    "        model_name = model_name+'-'+optimizer_name\n",
    "        model_directory =  os.path.join(experiment_dir, f'{model_name}.pt')\n",
    "        torch.save({\n",
    "            'model': model.state_dict()\n",
    "        }, model_directory)\n",
    "\n",
    "        print(f\"Model saved to checkpoint: {model_directory}\")\n",
    "\n",
    "    return test_accuracy * 100"
   ],
   "id": "ba1c6006-ebcd-4656-aade-2637a7ee620b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat all the previous for the **CIFAR-100** dataset using the **ResNet** model with different optimizers."
   ],
   "id": "13756640-0913-4e00-a541-df7980f183ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Resnet model on full CIFAR-100 data with random initialization and SGD optimizer\n",
    "random_init = train_model_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                       optimizer_name='sgd', model_name='resnet18')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_model_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99, optimizer_name='sgd',\n",
    "                   model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with warm-starting and SGD optimizer\n",
    "warm_start = train_model_threshold(title='resnet-sgd-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-sgd-cifar100/resnet18-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start ResNet models using SGD optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['resnet-sgd-cifar100'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with random initialization and Adam optimizer\n",
    "random_init = train_model_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                            optimizer_name='adam', model_name='resnet18')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_model_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train Resnet model on full CIFAR-100 data with warm-starting and Adam optimizer\n",
    "warm_start = train_model_threshold(title='resnet-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-adam-cifar100/resnet18-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start ResNet models using Adam optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['resnet-adam-cifar100'] = [random_init, warm_start, diff]"
   ],
   "id": "fc2a05fe-d04b-4e43-bb1d-07eca9d0d1de"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We repeat all the previous for the **CIFAR-100** dataset using the **MLP** model with different optimizers."
   ],
   "id": "ce8087aa-1cda-4747-959f-abeb2696501a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full CIFAR-100 data with random initialization and SGD optimizer\n",
    "random_init = train_model_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                                             optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_model_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99,\n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with warm-starting and SGD optimizer\n",
    "warm_start = train_model_threshold(title='mlp-sgd-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='mlp', \n",
    "                                checkpoint='experiments/exp2/mlp-sgd-cifar100/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['mlp-sgd-cifar100'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with random initialization and Adam optimizer\n",
    "random_init = train_model_threshold(title='mlp-adam-cifar100', dataset='cifar100',train_threshold=0.99,\n",
    "                                              optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train on half CIFAR-100 data\n",
    "_ = train_model_threshold(title='mlp-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full CIFAR-100 data with warm-starting and Adam optimizer\n",
    "warm_start = train_model_threshold(title='mlp-adam-cifar100', dataset='cifar100', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp', \n",
    "                                checkpoint='experiments/exp2/mlp-adam-cifar100/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on CIFAR-100\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['mlp-adam-cifar100'] = [random_init, warm_start, diff]"
   ],
   "id": "ee5fab6e-763d-45ce-af9b-8d0343e8003a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We create the previous for the **SVHN** dataset using the **ResNet** model with different optimizers."
   ],
   "id": "62c631d9-3153-4939-90b1-5ea0d32bdb45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ResNet model on full SVHN data with random initialization and SGD optimizer\n",
    "random_init = train_model_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                       optimizer_name='sgd', model_name='resnet18')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_model_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train ResNet model on full SVHN data with warm-starting and SGD optimizer\n",
    "warm_start = train_model_threshold(title='resnet-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='resnet18',\n",
    "                                checkpoint='experiments/exp2/resnet-sgd-svhn/resnet18-sgd.pt')\n",
    "\n",
    "# store the difference between random and warm-start ResNet models using SGD optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['resnet-sgd-svhn'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train ResNet model on full SVHN data with random initialization and Adam optimizer\n",
    "random_init = train_model_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                        optimizer_name='adam', model_name='resnet18')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_model_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='resnet18', use_half_data=True)\n",
    "\n",
    "# train ResNet model on full SVHN data with warm-starting and Adam optimizer\n",
    "warm_start = train_model_threshold(title='resnet-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='resnet18', \n",
    "                                checkpoint='experiments/exp2/resnet-adam-svhn/resnet18-adam.pt')\n",
    "\n",
    "# store the difference between random and warm-start ResNet models using Adam optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['resnet-adam-svhn'] = [random_init, warm_start, diff]"
   ],
   "id": "ed4c09b1-7ec1-496d-b122-03b23b905c8d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We create the previous for the **SVHN** dataset using the **MLP** model with different optimizers."
   ],
   "id": "2ba12e85-9e32-4ca2-bf95-ff90eb8b7a39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train MLP model on full SVHN data with random initialization and SGD optimizer\n",
    "random_init = train_model_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                             optimizer_name='sgd', model_name='mlp')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_model_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='sgd', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full SVHN data with warm-starting and SGD optimizer\n",
    "warm_start = train_model_threshold(title='mlp-sgd-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='sgd', model_name='mlp',\n",
    "                                checkpoint='experiments/exp2/mlp-sgd-svhn/mlp-sgd.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using SGD optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['mlp-sgd-svhn'] = [random_init, warm_start, diff]\n",
    "\n",
    "# train MLP model on full SVHN data with random initialization and Adam optimizer\n",
    "random_init = train_model_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99,\n",
    "                                              optimizer_name='adam', model_name='mlp')\n",
    "\n",
    "# train on half SVHN data\n",
    "_ = train_model_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                       optimizer_name='adam', model_name='mlp', use_half_data=True)\n",
    "\n",
    "# train MLP model on full SVHN data with warm-starting and Adam optimizer\n",
    "warm_start = train_model_threshold(title='mlp-adam-svhn', dataset='svhn', train_threshold=0.99, \n",
    "                                optimizer_name='adam', model_name='mlp',\n",
    "                                checkpoint='experiments/exp2/mlp-adam-svhn/mlp-adam.pt')\n",
    "\n",
    "# get the difference between random and warm-start MLP models using Adam optimizer on SVHN\n",
    "diff = random_init - warm_start\n",
    "\n",
    "# Store the results in the dictionary\n",
    "overall_results['mlp-adam-svhn'] = [random_init, warm_start, diff]"
   ],
   "id": "a3cd5e22-1e00-45b9-bb92-594ea099fa59"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "We save all the previous results in the `overall_results` dictionary and save it in `overall_results.json` to be loaded for table creation."
   ],
   "id": "e14bc9bd-032b-45d3-80c9-af966261a301"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp2/overall_results.json\", \"w\") as f:\n",
    "    json.dump(overall_results, f)"
   ],
   "id": "6361f6c5-0e0a-48f2-a81d-dac8088ea888"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The table is created in the next cell so we can compare our results with the table from second claims."
   ],
   "id": "00e998b4-fff5-4318-a2af-6911b00c1ae7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from json file\n",
    "with open(\"experiments/exp2/overall_results.json\", \"r\") as f:\n",
    "    overall_results = json.load(f)\n",
    "\n",
    "# Create a dataframe with the result to be in a table form\n",
    "df = pd.DataFrame.from_dict(overall_results).rename(index={0: \"Random Init\", 1: \"Warm Start\", 2: \"Difference\"})\n",
    "\n",
    "# Display the dataframe\n",
    "display(df.style.set_properties(**{'text-align': 'center', 'border': '1px solid black', 'padding': '5px'}))"
   ],
   "id": "529f1880-fd4b-457b-ae10-1d2201eace93"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "**Do the results validate the qualitative claim? Do the numerical values match the ones in the original paper? ðŸ¤”**\n",
    "\n",
    "**In the parameter table we speicified the parameter values that we used in the experiment. Can you find these values in the paper? ðŸ”**  \n",
    "Hint: one of the parameters is the x-label of one of the figure in the paper, mention the figure number.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "9e80322d-42a8-49e5-b7b9-98d2a221eb22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try: ðŸ§ª\n",
    "\n",
    "In this experiment you can:\n",
    "\n",
    "-   Change the learning rate by setting `lr=0.0001` as an argument in the `train_model_threshold` function\n",
    "-   Experiment with different `train_threshold` values and see how they affect the training time and the generalization gap\n",
    "-   Check the sensitivity of the model to the random seed by changing it\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "c0bac042-c876-4eab-a882-30aede1e2c69"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
