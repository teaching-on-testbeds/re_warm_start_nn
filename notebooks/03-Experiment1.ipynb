{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "In this section, we will evaluate the claims made by the authors. You should already know the general steps for each experiment from the previous section. We will now implement these experiments following the author description of each experiment and try to identify what was clear and what was vague due to incomplete information from the authors.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "f99ec320-f6fc-41cc-b98d-b85126fad7ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1:\n",
    "\n",
    "This experiment will test the claim that *‚ÄúWarm-starting neural network training can lead to lower test accuracy than random initialized models, even if they have similar final training accuracy‚Äù*. We anticipate that there will be a generalization gap between the two models trained with the two initialization methods.\n",
    "\n",
    "We compare two ways of training a ResNet-18 model, which is a type of deep neural network that can classify images. The CIFAR-10 dataset is a collection of 60,000 color images of 10 classes, such as airplanes, cars, and dogs. The experiment splits the dataset into two parts: a training set and a test set. The training set is used to update the model weights, and the test set is used to evaluate the model performance.\n",
    "\n",
    "The experiment uses two models:\n",
    "\n",
    "-   The warm-starting model starts with some pre-trained weights that are learned by training the model on 50% of the training data.\n",
    "-   The randomly initialized model starts with random weights that are not learned from any data.\n",
    "\n",
    "Both models train on the full training data for 350 epochs, where one epoch means one pass over the entire data. The experiment will measure the accuracy of the models on both the training and test sets, which is the percentage of correctly classified images."
   ],
   "id": "804ad140-74da-478d-b9aa-cc8bccf68eb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms, datasets, models"
   ],
   "id": "c03ae034-6b44-4aa7-85be-993205e060cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following function is `get_loaders` which we use to load the CIFAR-10 dataset, which consists of 60,000 color images of 10 classes, and returns data loaders for training and testing. The function has three parameters, you will need to know the following about them:\n",
    "\n",
    "-   `use_half_train`: a boolean flag that indicates whether to use only half of the training data or the whole dataset. If this is set to `True`, then the parameter `dataset_portion` is automatically set to 0.5.\n",
    "-   `dataset_portion`: a double value between 0 and 1 that indicates the portion of the training data to use. For example, if this is set to 0.8, then only 80% of the training data will be used and the rest will be discarded.\n",
    "\n",
    "The function returns a dictionary with two keys: `train_loader` and `test_loader` which can be used to iterate over the training and testing data respectively. The function also downloads the dataset from torchvision datasets if it is not already present in the specified directory."
   ],
   "id": "836130d9-10ff-43b7-a5df-de5a1fd6b322"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(use_half_train=False, batch_size=128, dataset_portion=None):\n",
    "    \"\"\"\n",
    "    This loads the whole CIFAR-10 into memory and returns train and test data according to params\n",
    "    @param use_half_train (bool): return half the data or the whole train data\n",
    "    @param batch_size (int): batch size for training and testing\n",
    "    @param dataset_portion (double): portion of train data\n",
    "\n",
    "    @returns dict() with train and test data loaders with keys `train_loader`, `test_loader`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalization using channel means\n",
    "    normalize_transform = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "    # Creating transform function\n",
    "    train_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "        \n",
    "    # Test transformation function    \n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize_transform])\n",
    "    \n",
    "    # Load data from torchvision datasets\n",
    "    original_train_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                         train=True, transform=train_transform, download=True)\n",
    "    original_test_dataset = datasets.CIFAR10(root=os.path.join('data', 'cifar10_data'),\n",
    "                                         train=False, transform=test_transform, download=True)\n",
    "    \n",
    "    # Check half data flag\n",
    "    if use_half_train:\n",
    "        print('Using Half Data')\n",
    "        dataset_portion = 0.5\n",
    "        \n",
    "    # Check if only a portion is required\n",
    "    if dataset_portion:\n",
    "        dataset_size = len(original_train_dataset)\n",
    "        split = int(np.floor((1 - dataset_portion) * dataset_size))\n",
    "        original_train_dataset, _ = random_split(original_train_dataset, [dataset_size - split, split])\n",
    "    \n",
    "    # Creating data loaders\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_train_dataset,\n",
    "        shuffle=True,\n",
    "        **loader_args)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=original_test_dataset,\n",
    "        shuffle=False,\n",
    "        **loader_args)\n",
    "\n",
    "    return {\"train_loader\": train_loader,\n",
    "            \"test_loader\": test_loader}"
   ],
   "id": "1040b309-5047-4c37-a889-bb0cb256582f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "The following function is the `train_model_epochs` which trains a ResNet-18 model on the CIFAR-10 dataset and returns the train and test accuracies. The function takes six parameters:\n",
    "\n",
    "-   `title`: a string that specifies the name of the experiment. This is used to create a subdirectory under the `experiments/exp1` directory where the model checkpoints and final weights will be saved.\n",
    "-   `experiment_dir`: a string that specifies the path of the experiment directory. If this is `None`, then the function will use the title parameter to create a default directory name.\n",
    "-   `use_half_data`: a boolean flag that indicates whether to use half of the training data or the whole dataset. This is passed to the `get_loaders` function that loads the data loaders.\n",
    "-   `lr`: a float value that specifies the learning rate for the stochastic gradient descent optimizer.\n",
    "-   `checkpoint`: a string that specifies the path of a model checkpoint file. If this is not `None`, then the function will load the model weights from the checkpoint file and resume training from there.\n",
    "-   `epochs`: an integer that specifies the number of epochs to train the model for.\n",
    "\n",
    "The function returns a tuple of two lists: `train_acc` and `test_acc`. These are lists that contain the train and test accuracies for each epoch, respectively. The function uses the [ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) model from the torchvision models. The function also sets the random seeds for reproducibility. The function uses cross entropy loss as the loss function and SGD as an optimizer. The function also uses a helper function called get_accuracy to compute the accuracy of the model predictions."
   ],
   "id": "662e3354-5c0c-450c-a0c7-1d5f192677ca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes predictions and true values to return accuracies\n",
    "def get_accuracy(logit, true_y):\n",
    "    pred_y = torch.argmax(logit, dim=1)\n",
    "    return (pred_y == true_y).float().mean()\n",
    "\n",
    "# Function to train the model and return train and test accuracies\n",
    "def train_model_epochs(title='', experiment_dir=None, use_half_data=False,\n",
    "                        lr=0.001, checkpoint=None, epochs=10, random_seed=42):\n",
    "    \n",
    "    # Create experiment directory name if none\n",
    "    if experiment_dir is None:\n",
    "        experiment_dir = os.path.join('experiments/exp1', title)\n",
    "\n",
    "    # make experiment directory\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"CUDA Recognized\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    # Get the dataset\n",
    "    loaders = get_loaders(use_half_train=use_half_data)\n",
    "\n",
    "    # Get the model\n",
    "    model = models.resnet18(num_classes=10).to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Create the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get model from checkpoint\n",
    "    if checkpoint is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint, map_location=device)['model'])\n",
    "\n",
    "    # Arrays to hold accuracies\n",
    "    test_acc = [0]\n",
    "    train_acc = [0]\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        # Calculate loss and gradients for models on every training batch\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"train_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model_y = model(data_x)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "            \n",
    "            # Perform back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Store training accuracy for plotting\n",
    "        train_loss = np.mean(losses)\n",
    "        train_accuracy = np.mean(accuracies)\n",
    "        train_acc.append(train_accuracy*100)\n",
    "\n",
    "        print(\"Train accuracy: {} Train loss: {}\".format(train_accuracy, train_loss))\n",
    "\n",
    "        # Evaluate the model on all the test batches\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        for batch_idx, (data_x, data_y) in enumerate(loaders[\"test_loader\"]):\n",
    "            data_x = data_x.to(device)\n",
    "            data_y = data_y.to(device)\n",
    "\n",
    "            model_y = model(data_x)\n",
    "            loss = criterion(model_y, data_y)\n",
    "            batch_accuracy = get_accuracy(model_y, data_y)\n",
    "\n",
    "            accuracies.append(batch_accuracy.item())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Store test accuracy for plotting\n",
    "        test_loss = np.mean(losses)\n",
    "        test_accuracy = np.mean(accuracies)\n",
    "        test_acc.append(test_accuracy*100)\n",
    "        print(\"Test accuracy: {} Test loss: {}\".format(test_accuracy, test_loss))\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save({\n",
    "        'model': model.state_dict()\n",
    "    }, os.path.join(experiment_dir, 'final.pt'))\n",
    "    \n",
    "    # return the accuracies\n",
    "    return train_acc, test_acc"
   ],
   "id": "677fbfed-b30d-407b-94ac-2c16d49bc00c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Before running the experiment we create a parameter table to store the parameter values from the paper that we will use in the upcoming cells.\n",
    "\n",
    "|        Model         | Learning rate | Epochs | use half data |      Checkpoint      | Optimizer |\n",
    "|:----------------:|:----------:|:----:|:----------:|:---------------:|:-------:|\n",
    "| Trained on half data |    0.0001     |  350   |     True      |    No checkpoint     |    SGD    |\n",
    "|    Warm-Starting     |    0.0001     |  350   |     False     | Trained on half data |    SGD    |\n",
    "|  Random initialized  |    0.0001     |  350   |     False     |    No checkpoint     |    SGD    |"
   ],
   "id": "a5104aab-5600-446c-b7a4-71c07ce38cc1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "To be used warm-starting a model later, we first train a model for 350 epochs on 50% of the CIFAR-10 dataset. We keep track of the train and test accuracies at each epoch, which will form the blue line on the left half of figure 1.\n",
    "\n",
    "We set `use_half_data` to `True` to train on only half of the CIFAR-10 dataset. We don‚Äôt need a `checkpoint` since we start from scratch."
   ],
   "id": "46256dc6-9909-47e3-8d5a-aecad845011a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize runs dictionary to hold runs outputs\n",
    "runs = {}\n",
    "\n",
    "# Run the train_model_epochs function to get train and test accuracies for the first model \n",
    "# Random initialized model trained on half the data\n",
    "half_train_acc, half_test_acc = train_model_epochs( title=\"half_cifar\",\n",
    "                                                    use_half_data=True,\n",
    "                                                    lr=0.001,\n",
    "                                                    checkpoint=None,\n",
    "                                                    epochs=350 )\n",
    "\n",
    "# Put the results in the runs dictionary\n",
    "runs[\"half_cifar\"] = { 'training_accuracy' : half_train_acc,\n",
    "                       'test_accuracy' : half_test_acc,\n",
    "                       'offset' : 0\n",
    "                     }"
   ],
   "id": "7556e02e-7a40-497a-b00b-d4b78a0fa8fa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now we use the previous model to train a warm-starting model for 350 epochs on 100% of the CIFAR-10 dataset. We keep track of the train and test accuracies at each epoch, which will form the blue line on the right half of figure 1.\n",
    "\n",
    "We set `use_half_data` to `False` to train on the full CIFAR-10 dataset. We specify a `checkpoint` to the model trained on 50% of the data."
   ],
   "id": "5cfda1b2-59bc-469f-81d4-d29c66c54162"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the train_model_epochs function to get train and test accuracies for the Second model\n",
    "# Warm starting model using the first model\n",
    "ws_train_acc, ws_test_acc = train_model_epochs( title=\"warm_start\",\n",
    "                                                use_half_data=False,\n",
    "                                                lr=0.001,\n",
    "                                                checkpoint='experiments/exp1/half_cifar/final.pt',\n",
    "                                                epochs=350 )\n",
    "\n",
    "# Put the results in the runs dictionary\n",
    "runs[\"warm_start\"] = { 'training_accuracy' : ws_train_acc,\n",
    "                       'test_accuracy' : ws_test_acc,\n",
    "                       'offset' : len(ws_test_acc)\n",
    "                     }"
   ],
   "id": "27b75bff-7c37-48c3-99d8-dbc3457b7ff1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Finaly, we train a model for 350 epochs on 100% of the CIFAR-10 dataset. We keep track of the train and test accuracies at each epoch, which will form the orange line on the right half of figure 1.\n",
    "\n",
    "We set `use_half_data` to `False` to train on the full CIFAR-10 dataset. We don‚Äôt need a `checkpoint` since we start from scratch."
   ],
   "id": "92d8acb7-c662-44b1-8e34-64a94975a2a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the train_model_epochs function to get train and test accuracies for the Last model\n",
    "# Model with random initialization\n",
    "ri_train_acc, ri_test_acc = train_model_epochs( title=\"random_init\",\n",
    "                                                use_half_data=False,\n",
    "                                                lr=0.001,\n",
    "                                                checkpoint=None,\n",
    "                                                epochs=350 )\n",
    "\n",
    "# Put the results in the runs dictionary\n",
    "runs[\"random_init\"] = { 'training_accuracy' : ri_train_acc,\n",
    "                       'test_accuracy' : ri_test_acc,\n",
    "                       'offset' : len(ri_test_acc)\n",
    "                     }"
   ],
   "id": "305198e6-aea2-40d7-8eba-7decbfa7179e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Now we save the training and test accuracies in the runs dictionary in `runs.json`."
   ],
   "id": "d62e01c0-8ea2-421a-a98b-46688911b976"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs in a json file\n",
    "with open(\"experiments/exp1/runs.json\", \"w\") as f:\n",
    "    json.dump(runs, f)"
   ],
   "id": "8a3b41f3-bb4d-4a56-bedb-6ee0ac3d54d3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Let‚Äôs visualize the accuracies and analyze the outcomes! Run the next cell to plot the accuracies."
   ],
   "id": "2cd65914-3307-4a89-9b92-289025817477"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from json file\n",
    "with open(\"experiments/exp1/runs.json\", \"r\") as f:\n",
    "    runs = json.load(f)\n",
    "    \n",
    "# Get number of epochs\n",
    "epochs = len(runs['half_cifar']['training_accuracy'])\n",
    "\n",
    "# Select colors\n",
    "colors = {'half_cifar' : 'C0',\n",
    "          'warm_start' : 'C0',\n",
    "          'random_init' : 'C1',\n",
    "         }\n",
    "\n",
    "# Plot train Figure\n",
    "plt.figure()\n",
    "for title, dictionary in runs.items():\n",
    "    offset = dictionary['offset']\n",
    "    x = np.arange(offset, offset + len(dictionary['training_accuracy']))\n",
    "    y = dictionary['training_accuracy']\n",
    "    plt.plot(x, y, label=title, c=colors[title])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\" Train accuracy \")\n",
    "plt.ylim(0, 100)\n",
    "plt.plot([epochs, epochs], plt.gca().get_ylim(), '--', c='black')\n",
    "plt.savefig(\"experiments/exp1/fig1_train.png\")\n",
    "\n",
    "# Plot test Figure\n",
    "plt.figure()\n",
    "for title, dictionary in runs.items():\n",
    "    offset = dictionary['offset']\n",
    "    x = np.arange(offset, offset + len(dictionary['test_accuracy']))\n",
    "    y = dictionary['test_accuracy']\n",
    "    plt.plot(x, y, label=title, c=colors[title])\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(\" Test accuracy \")\n",
    "plt.ylim(0, 100)\n",
    "plt.plot([epochs, epochs], plt.gca().get_ylim(), '--', c='black')\n",
    "plt.savefig(\"experiments/exp1/fig1_test.png\")"
   ],
   "id": "e5146ed0-8dfd-46d2-ae81-134a441f364e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "**Did we validate the qualitative claim? Numerically, are the results consistent with the original paper? ü§î**\n",
    "\n",
    "**In the parameter table we speicified the parameter values that we used in the experiment. Can you find these values in the paper text? üîç**\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "f3d8a7dd-e9fe-4fc6-8bf2-f670c81382a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try:\n",
    "\n",
    "This experiment uses a specific model and optimizer. Exploring different combinations might be beneficial but costly in terms of computation. A simple way to further examine the first claim is:\n",
    "\n",
    "-   Use a lower learning rate since the model achieves 99% training accuracy quickly\n",
    "-   Use number of epochs at which validation accuracies of both models are maximized\n",
    "-   Check the sensitivity of the model to the random seed by changing it\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "0e3a453a-1495-4754-afc5-7cf1f4005f59"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
